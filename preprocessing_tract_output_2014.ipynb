{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This loads and integrates PLUTO, DOB/ECB, and ACS data and aggregates to the census tract level. It truncates the data to include 2014 data, which impacts DOB/ECB datasets.\n",
    "\n",
    "# Use this output of tracts by features (3180 x 718 matrix) to cross-validate with gas leaks as trained on 2013 data. Use features to predict 2015 gas leaks for final model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import pickle\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables for data pathfiles\n",
    "\n",
    "FDNY_TRACT_13 = 'processed_data/tract_output/tract_incidents_2013.csv'\n",
    "FDNY_TRACT_14 = 'processed_data/tract_output/tract_incidents_2014.csv'\n",
    "FDNY_TRACT_15 = 'processed_data/tract_output/tract_incidents_2015.csv'\n",
    "PLUTO_BK = 'raw_data/PLUTO/Brooklyn/BKMapPLUTO.shp'\n",
    "PLUTO_BX = 'raw_data/PLUTO/Bronx/BXMapPLUTO.shp'\n",
    "PLUTO_MN = 'raw_data/PLUTO/Manhattan/MNMapPLUTO.shp'\n",
    "PLUTO_QN = 'raw_data/PLUTO/Queens/QNMapPLUTO.shp'\n",
    "PLUTO_SI = 'raw_data/PLUTO/Staten_Island/SIMapPLUTO.shp'\n",
    "MASTER_PLUTO_PICKLE_BBL = 'processed_data/master_pluto_bbl.pickle'\n",
    "DOB_COMPLAINTS = 'raw_data/DOB_Complaints_Received.csv'\n",
    "DOB_ECB = 'raw_data/DOB_ECB_Violations.csv'\n",
    "DOB_VIOLATIONS = 'raw_data/DOB_Violations.csv'\n",
    "DOB_PERMITS = 'raw_data/Historical_DOB_Permit_Issuance.csv'\n",
    "BIN_BBL = 'raw_data/building_0117.csv'\n",
    "PAD = 'raw_data/PAD/bobaadr.txt'\n",
    "CENSUS_TRACT_RACE = 'raw_data/CENSUS_TRACT_RACE_INCOME/ACS_15_5YR_DP05_with_ann.csv'\n",
    "CENSUS_TRACT_INCOME = 'raw_data/CENSUS_TRACT_RACE_INCOME/ACS_15_5YR_S1901_with_ann.csv'\n",
    "NYC_TRACTS = 'raw_data/nyc_tract/nyct2010.shp'\n",
    "ZIP_TRACTS = 'raw_data/zip_tract_122015.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FDNY data per tract -- taken from an earlier spatial join with street polygons\n",
    "- 2014 FDNY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import FDNY data\n",
    "fdny = pd.read_csv(FDNY_TRACT_14)\n",
    "fdny['TRACT'] = fdny['GEOID'].astype(str)\n",
    "del fdny['GEOID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC zip code and census tract shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import NYC census tract shapefiles\n",
    "nyc_tracts = gp.read_file(NYC_TRACTS)\n",
    "\n",
    "# boro code to TRACT\n",
    "boro_to_ct = {'1':'36061','2':'36005','3':'36047','4':'36081','5':'36085'}\n",
    "NYC_st_ct = [v for k,v in boro_to_ct.items()]\n",
    "\n",
    "def boro2stct(data):\n",
    "    return boro_to_ct[data]\n",
    "\n",
    "nyc_tracts['TRACT'] = nyc_tracts['BoroCode'].apply(lambda x: boro2stct(x))+nyc_tracts['CT2010'].astype(str)\n",
    "\n",
    "# read in zip_tract\n",
    "zip_tracts = pd.read_csv(ZIP_TRACTS,usecols=['ZIP','TRACT'],dtype={'ZIP':str,'TRACT':str})\n",
    "zip_tracts.rename(columns = {'ZIP':'ZipCode'},inplace=True)\n",
    "\n",
    "# merge to get NYC zips only\n",
    "zip_tracts_nyc = nyc_tracts.merge(zip_tracts,how='left',on='TRACT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC PLUTO (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def import_filter_pluto():\n",
    "    \n",
    "    # import PLUTO for 5 boros\n",
    "    BK = gp.read_file(PLUTO_BK)\n",
    "    BX = gp.read_file(PLUTO_BX)\n",
    "    MN = gp.read_file(PLUTO_MN)\n",
    "    QN = gp.read_file(PLUTO_QN)\n",
    "    SI = gp.read_file(PLUTO_SI)\n",
    "    \n",
    "    # merge 5 boro PLUTO datasets \n",
    "    pluto_agg = BK.append(BX)\n",
    "    pluto_agg = pluto_agg.append(MN)\n",
    "    pluto_agg = pluto_agg.append(QN)\n",
    "    pluto_agg = pluto_agg.append(SI)\n",
    "    \n",
    "    # select key columns\n",
    "\n",
    "    pluto_select = pluto_agg[['ZipCode',\n",
    "    'BBL',\n",
    "    'Tract2010',\n",
    "    'BldgClass',\n",
    "    'LandUse',\n",
    "    'BldgArea',\n",
    "    'ComArea',\n",
    "    'ResArea',\n",
    "    'OfficeArea',\n",
    "    'RetailArea',\n",
    "    'UnitsRes',\n",
    "    'UnitsTotal',\n",
    "    'AssessTot',\n",
    "    'YearBuilt',\n",
    "    'BuiltFAR','LotArea']]\n",
    "    \n",
    "    # create pickle\n",
    "\n",
    "    with open(MASTER_PLUTO_PICKLE_BBL, 'wb') as handle:\n",
    "        pickle.dump(pluto_select, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Loading pickle...\n"
     ]
    }
   ],
   "source": [
    "# ***********************\n",
    "# NOTE: Since Geopandas does not allow filtering select columns, \n",
    "# you'll have to load full PLUTO set, merge, and select columns,\n",
    "# then save as a pickle file for later use.\n",
    "# ***********************\n",
    "\n",
    "if os.path.exists(MASTER_PLUTO_PICKLE_BBL):\n",
    "    print \"File exists. Loading pickle...\"\n",
    "    # load pickle of PLUTO data\n",
    "    with open(MASTER_PLUTO_PICKLE_BBL, 'rb') as handle:\n",
    "        master_pluto = pickle.load(handle)\n",
    "    \n",
    "else:\n",
    "    print \"File does not yet exist. Importing and filtering PLUTO. This could take several minutes...\"\n",
    "    # first time only, import, filter, and save processed PLUTO as a pickle for future use\n",
    "    import_filter_pluto()\n",
    "    \n",
    "    # load pickle of PLUTO data\n",
    "    with open(MASTER_PLUTO_PICKLE_BBL, 'rb') as handle:\n",
    "        master_pluto = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOB and ECB permits and violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of building bins and BBLs\n",
    "bin_bbl = pd.read_csv(BIN_BBL,usecols=['BBL','BIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DOB complaints\n",
    "dob_complaints = pd.read_csv(DOB_COMPLAINTS,usecols=['Complaint Number', 'Date Entered', \n",
    "                                 'BIN', 'Complaint Category', \n",
    "                                 'Disposition Date','Disposition Code', \n",
    "                                 'Inspection Date'])\n",
    "\n",
    "# merge BBL to the dob dataset\n",
    "dob_complaints_bbl = dob_complaints.merge(bin_bbl,how='inner',on='BIN')\n",
    "dob_complaints_bbl['BBL'] = dob_complaints_bbl['BBL'].astype(int).astype(str)\n",
    "\n",
    "# remove handful of outlier BBLs\n",
    "dob_bbl_indx = dob_complaints_bbl['BBL'].apply(lambda x: len(str(x))==10)\n",
    "dob_complaints_bbl = dob_complaints_bbl[dob_bbl_indx].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DOB violations \n",
    "dob_violations = pd.read_csv(DOB_VIOLATIONS,dtype={'BORO':str,\n",
    "                                                   'BLOCK':str,'LOT':str,\n",
    "                                                   'ISSUE_DATE':str,\n",
    "                                                   'DISPOSITION_DATE':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# ECB violations\n",
    "ecb = pd.read_csv(DOB_ECB,usecols=['BIN','BORO','BLOCK','LOT','SEVERITY','VIOLATION_TYPE',\n",
    "                                   'VIOLATION_DESCRIPTION',\n",
    "                                   'INFRACTION_CODE1','ISSUE_DATE',\n",
    "                                   'SECTION_LAW_DESCRIPTION1'],dtype={'BORO':str,\n",
    "                                                                      'BLOCK':str,\n",
    "                                                                      'LOT':str,\n",
    "                                                                      'ISSUE_DATE':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DOB work permits\n",
    "permits = pd.read_csv(DOB_PERMITS,usecols=['Zip Code','BOROUGH','Block','Lot',\n",
    "                                           'Bldg Type','Residential','Permit Type',\n",
    "                                           'Oil Gas','Issuance Date'],dtype={'BOROUGH':str,\n",
    "                                                                           'Block':str,\n",
    "                                                                           'Lot':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# race data\n",
    "race = pd.read_csv(CENSUS_TRACT_RACE,skiprows=[1],usecols=['GEO.id2', 'HC01_VC03', \n",
    "                                                           'HC01_VC49', 'HC01_VC50', \n",
    "                                                           'HC01_VC51','HC01_VC56', \n",
    "                                                           'HC01_VC64', 'HC01_VC69', 'HC01_VC23'])\n",
    "\n",
    "# rename columns\n",
    "race.rename(columns={'HC01_VC03': 'TOTAL_POPULATION', 'HC01_VC49': 'WHITE',\n",
    "                        'HC01_VC50': 'BLACK_AFRICAN_AMERICAN', 'HC01_VC51': 'AMERICAN_INDIAN_AND_ALASKA_NATIVE',\n",
    "                        'HC01_VC56': 'ASIAN', 'HC01_VC64': 'NATIVE_HAWAIIAN_AND_OTHER_PACIFIC_ISLANDER',\n",
    "                        'HC01_VC69': 'SOME_OTHER_RACE', 'HC01_VC23': 'MEDIAN_AGE', 'GEO.id2': 'GEOID'}, inplace=True)\n",
    "\n",
    "# convert values to float\n",
    "def make_float(expected_float):\n",
    "    try:\n",
    "        if type(expected_float) == str:\n",
    "            expected_float = expected_float.replace('+', '').replace(',', '')\n",
    "        return float(expected_float)\n",
    "    except:\n",
    "        # print expected_float\n",
    "        return np.nan\n",
    "\n",
    "for i in race.columns[race.columns!='GEOID']:\n",
    "    race[i] = race[i].apply(lambda x: make_float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# income data\n",
    "income = pd.read_csv(CENSUS_TRACT_INCOME,skiprows=[1],usecols=['GEO.id2', 'HC01_EST_VC01', 'HC01_EST_VC15'])\n",
    "\n",
    "# rename columns\n",
    "income.rename(columns={'HC01_EST_VC01': 'TOTAL_HOUSEHOLDS', \n",
    "                       'HC01_EST_VC15': 'MEAN_INCOME', 'GEO.id2': 'GEOID'}, inplace=True)\n",
    "\n",
    "#convert values to float\n",
    "for i in income.columns[income.columns!='GEOID']:\n",
    "    income[i] = income[i].apply(lambda x: make_float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATE SELECTION: 2014 data (use to cross-validate and predict 2015 gas leaks)\n",
    "Time-sensitive datasets: FDNY,DOB/ECB\n",
    "\n",
    "We'll assume the same PLUTO and ACS variables stand for all years.\n",
    "\n",
    "** Note that the DOB Work Permits data does not extend past 2013, but we're including it given the assumption that historical work permits still apply to buildings existing in 2013-2015 and could be factors. (Exceptions clearly would occur to this assumption). So we will use the same historical data (work permits through 2013) for both training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep datatime fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dob/ecb date conversion\n",
    "dob_complaints_bbl['date_entered'] = dob_complaints_bbl['Date Entered'].apply(\n",
    "    lambda x: datetime.datetime.strptime(x,'%m/%d/%Y'))\n",
    "\n",
    "permits['issuance_date'] = permits['Issuance Date'].apply(\n",
    "    lambda x: datetime.datetime.strptime(x,'%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with inconsistent datetime\n",
    "def dob_date(data):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(data, '%Y%m%d')\n",
    "    except:\n",
    "        try:\n",
    "            y,md = data.split('  ')\n",
    "            if y in ['11','12','13','14','15']:\n",
    "                data = '20'+y+md\n",
    "                return datetime.datetime.strptime(data, '%Y%m%d')\n",
    "        except:\n",
    "            try:\n",
    "                data = str(data)[:8]\n",
    "                return datetime.datetime.strptime(data, '%Y%m%d')\n",
    "            except:\n",
    "                return float(\"NaN\")\n",
    "            \n",
    "dob_violations['issue_date'] = dob_violations['ISSUE_DATE'].apply(\n",
    "    lambda x: dob_date(x))\n",
    "\n",
    "# cut out records with incoherent date format or years before 2013\n",
    "dob_violations = dob_violations[~dob_violations['issue_date'].isnull()]\n",
    "dob_violations['issue_date'] = dob_violations['issue_date'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with inconsistent datetime\n",
    "def ecb_date(data):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(str(data), '%Y%m%d')\n",
    "    except:\n",
    "        return float(\"NaN\")\n",
    "\n",
    "ecb['issue_date'] = ecb['ISSUE_DATE'].apply(\n",
    "    lambda x: ecb_date(x))\n",
    "\n",
    "# cut out the 85 records with incoherent date format\n",
    "ecb = ecb[~ecb['issue_date'].isnull()]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set dates for 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# date filter method\n",
    "def filter_date(data, date_col):\n",
    "    start_date = datetime.date(2014,1,1)\n",
    "    end_date = datetime.date(2015,1,1) # not inclusive of end date\n",
    "        \n",
    "    # truncate to selected dates\n",
    "    return data[(data[date_col]>=start_date) & (data[date_col]<end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# segment data by date\n",
    "dob_complaints_sliced = filter_date(dob_complaints_bbl,'date_entered')\n",
    "dob_violations_sliced = filter_date(dob_violations,'issue_date')\n",
    "ecb_sliced = filter_date(ecb,'issue_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter, aggregate and scale data\n",
    "\n",
    "- Preprocess and scale PLUTO attributes\n",
    "- Aggregate PLUTO data by census tract\n",
    "- Preprocess and scale building data attributes\n",
    "- Aggregate building data by census tract\n",
    "- Aggregate census tract data by census tract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess and scale PLUTO attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge bin_bbl with pluto\n",
    "master_pluto['BBL_int'] = master_pluto['BBL'].astype(int)\n",
    "\n",
    "# concatenate full TRACT number\n",
    "master_pluto['st_ct_FIPS'] = master_pluto['BBL'].apply(lambda x: boro2stct(str(x)[:1]))\n",
    "master_pluto['ctract'] = master_pluto['Tract2010'].apply(lambda x: str(x)+'00' if len(x)==4 else str(x))\n",
    "\n",
    "master_pluto['TRACT'] = master_pluto['st_ct_FIPS']+master_pluto['ctract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate building age\n",
    "# note: to keep equal comparison b/t train and test we're keeping age as of 2013 for both\n",
    "def year_calc(data):\n",
    "    if (data < 1800) | (data > 2013):\n",
    "        return float('NaN')\n",
    "    else:\n",
    "        return 2013-data \n",
    "    \n",
    "master_pluto['age'] = master_pluto.YearBuilt.apply(lambda x: year_calc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_and_group_tract(data,tract_col_name,field,multiple=True,\n",
    "                        header_prefix=None,dispose=False,og=False):\n",
    "    '''Creates a general \"group by\" and scaling function that:\n",
    "    - groups data for a given variable category in an input dataframe by census tract\n",
    "    - then creates a ratio of each variable category in the tract for all values in tract\n",
    "    - produces a new sparse matrix with rows=census tracts and cols=each category of the variable,\n",
    "    where the values are the ratio of the category / all instances in the census tract\n",
    "    '''\n",
    "    # group selected variable by tract\n",
    "    if multiple:\n",
    "        if og:\n",
    "            data2 = data.copy()\n",
    "            data2[field] = data2[field].fillna('NA')\n",
    "            data = data2.copy()\n",
    "        \n",
    "        temp_df = data.groupby([tract_col_name,field])[\n",
    "            field].count().unstack(level=-1).reset_index()\n",
    "        \n",
    "        # create df of ratio of select category of variable per all instances in tract\n",
    "        tract_matrix = pd.DataFrame()\n",
    "\n",
    "        for i in range(len(temp_df[tract_col_name])):\n",
    "            tract_matrix[str(\n",
    "                temp_df[tract_col_name][i])] = temp_df.T[i][1:]/temp_df.T[i][1:].sum()\n",
    "    \n",
    "        tract_matrix = tract_matrix.T.reset_index()\n",
    "\n",
    "        tract_matrix['TRACT'] = tract_matrix['index'].astype(str)\n",
    "        if og:\n",
    "            tract_matrix = tract_matrix.drop(['index','NA'],axis=1)\n",
    "        else:\n",
    "            tract_matrix = tract_matrix.drop('index',axis=1)\n",
    "        \n",
    "        if dispose:\n",
    "            tract_matrix = tract_matrix.rename(columns={tract_matrix.columns[0]:'No_disposition'})\n",
    "\n",
    "        \n",
    "        # update header to specific source data (for less confusion when merging data later)\n",
    "        if header_prefix:\n",
    "            new_columns = []\n",
    "            for col in tract_matrix.columns:\n",
    "                if col != 'TRACT':\n",
    "                    new_columns.append(header_prefix+col)\n",
    "                else:\n",
    "                    new_columns.append(col)\n",
    "            tract_matrix.columns = new_columns\n",
    "            \n",
    "        return tract_matrix       \n",
    "    \n",
    "    else:\n",
    "    \n",
    "        return pd.DataFrame((data.groupby(tract_col_name)[field].sum())/data.groupby(\n",
    "                tract_col_name)[field].count()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# groupby tract: average age\n",
    "avg_bldg_age_by_tract = scale_and_group_tract(master_pluto,'TRACT','age',multiple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# groupby tract: # building class in tract / total building in tract\n",
    "bldgclass_by_tract = scale_and_group_tract(master_pluto,'TRACT','BldgClass',\n",
    "                                       multiple=True,header_prefix='bldg_class_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# groupby tract: # landuse in tract / total land uses in tract\n",
    "landuse_by_tract = scale_and_group_tract(master_pluto,'TRACT','LandUse',\n",
    "                                       multiple=True,header_prefix='landuse_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the following assorted pluto attributes are aggregated by zip\n",
    "pluto_attrib_by_tract = pd.DataFrame()\n",
    "\n",
    "# function to create ratio of given PLUTO category per zip code\n",
    "def pluto_attributes_tract(data,tract_col_name,oldfield,newfield,denominator='BldgArea'):\n",
    "    pluto_attrib_by_tract[newfield] = data.groupby(\n",
    "        tract_col_name)[oldfield].sum()*1.0/data.groupby(tract_col_name)[denominator].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# commercial ratio by tract\n",
    "pluto_attributes_tract(master_pluto,'TRACT','ComArea','com_ratio',denominator='BldgArea')\n",
    "\n",
    "# residential ratio by tract\n",
    "pluto_attributes_tract(master_pluto,'TRACT','ResArea','res_ratio',denominator='BldgArea')\n",
    "\n",
    "# office ratio by tract\n",
    "pluto_attributes_tract(master_pluto,'TRACT','OfficeArea','office_ratio',denominator='BldgArea')\n",
    "\n",
    "# retail ratio by tract\n",
    "pluto_attributes_tract(master_pluto,'TRACT','RetailArea','retail_ratio',denominator='BldgArea')\n",
    "\n",
    "# res / total units by tract\n",
    "pluto_attributes_tract(master_pluto,'TRACT','UnitsRes','res_unit_ratio',denominator='UnitsTotal')\n",
    "\n",
    "# mean unit area by tract\n",
    "pluto_attributes_tract(master_pluto,'TRACT','BldgArea','unit_area',denominator='UnitsTotal')\n",
    "\n",
    "# assessed value per sq foot\n",
    "pluto_attributes_tract(master_pluto,'TRACT','AssessTot','value_per_ft',denominator='LotArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total units by zip code\n",
    "pluto_attrib_by_tract['total_units'] = master_pluto.groupby('TRACT')['UnitsTotal'].sum()\n",
    "\n",
    "# reset index\n",
    "pluto_attrib_by_tract = pluto_attrib_by_tract.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess and scale the DOB and ECB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/Daynan/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# add zeros to even out block and code\n",
    "def add_zero(data):\n",
    "    if len(str(data))==4:\n",
    "        return '0'+str(data)\n",
    "    if len(str(data))==3:\n",
    "        return '00'+str(data)\n",
    "    elif len(str(data))==2:\n",
    "        return '000'+str(data)\n",
    "    elif len(str(data))==1:\n",
    "        return '0000'+str(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def add_zero_lt(data):\n",
    "    if len(str(data))==5:\n",
    "        return str(data)[1:]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def st_clip(data):\n",
    "    if len(str(data))>1:\n",
    "        return str(data)[:1]\n",
    "    else:\n",
    "        return str(data)    \n",
    "    \n",
    "dob_violations_sliced['block_'] = dob_violations_sliced['BLOCK'].apply(lambda x: add_zero(x))\n",
    "dob_violations_sliced['lot_'] = dob_violations_sliced['LOT'].apply(lambda x: add_zero_lt(x))\n",
    "\n",
    "ecb_sliced['BORO'] = ecb_sliced['BORO'].apply(lambda x: st_clip(x))\n",
    "ecb_sliced['block_'] = ecb_sliced['BLOCK'].apply(lambda x: add_zero(x))\n",
    "ecb_sliced['lot_'] = ecb_sliced['LOT'].apply(lambda x: add_zero_lt(x))\n",
    "\n",
    "# concatenate BBL\n",
    "dob_violations_sliced['BBL'] = dob_violations_sliced['BORO'].astype(\n",
    "    str)+dob_violations_sliced['BLOCK'].astype(str)+dob_violations_sliced['lot_'].astype(str)\n",
    "\n",
    "ecb_sliced['BBL'] = ecb_sliced['BORO'].astype(\n",
    "    str)+ecb_sliced['BLOCK'].astype(str)+ecb_sliced['lot_'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# permits dataset\n",
    "BOROUGH_to_ct = {'MANHATTAN':'1','BRONX':'2','BROOKLYN':'3','QUEENS':'4','STATEN ISLAND':'5'}\n",
    "\n",
    "permits['Boro'] = permits['BOROUGH'].apply(lambda x: BOROUGH_to_ct[x])\n",
    "\n",
    "permits['block_'] = permits['Block'].apply(lambda x: add_zero(x))\n",
    "permits['lot_'] = permits['Lot'].apply(lambda x: add_zero_lt(x))\n",
    "\n",
    "permits['BBL'] = permits['Boro'].astype(\n",
    "    str)+permits['block_'].astype(str)+permits['lot_'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zip to bbl\n",
    "zip_bbl = master_pluto[['TRACT','BBL']].copy()\n",
    "zip_bbl['BBL'] = zip_bbl['BBL'].astype(int).astype(str)\n",
    "\n",
    "# merge DOB/ECB to PLUTO\n",
    "dob_complaints_tract = dob_complaints_sliced.merge(zip_bbl,how='inner',on='BBL')\n",
    "\n",
    "ecb_tract = ecb_sliced.merge(zip_bbl,how='inner',on='BBL')\n",
    "\n",
    "dob_violations_tract = dob_violations_sliced.merge(zip_bbl,how='inner',on='BBL')\n",
    "\n",
    "permits_tract = permits.merge(zip_bbl,how='inner',on='BBL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process DOB/ECB data as with the PLUTO data (group by tract and scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same as above, group various DOB / ECB datasets to tract and scale\n",
    "\n",
    "# groupby tract: # complaint category in tract / total complaints in tract\n",
    "complaints_by_tract = scale_and_group_tract(dob_complaints_tract,'TRACT','Complaint Category',\n",
    "                                       multiple=True,header_prefix='DOB_complaint_')\n",
    "\n",
    "# groupby tract: # disposition code in tract / total dispositions in tract\n",
    "disposition_by_tract = scale_and_group_tract(dob_complaints_tract,'TRACT','Disposition Code',\n",
    "                                       multiple=True,header_prefix='DOB_dispos_',dispose=True)\n",
    "\n",
    "# groupby tract:  # dob violations type in tract / total violations in tract\n",
    "violations_by_tract = scale_and_group_tract(dob_violations_tract,'TRACT','VIOLATION_TYPE',\n",
    "                                       multiple=True,header_prefix='DOB_violation_')\n",
    "\n",
    "# groupby tract: # ecb violation type in tract / total violations in tract\n",
    "ecb_violations_by_tract = scale_and_group_tract(ecb_tract,'TRACT','VIOLATION_TYPE',\n",
    "                                      multiple=True,header_prefix='ECB_violation_')\n",
    "\n",
    "# groupby tract: # ecb violation type in tract / total violations in tract\n",
    "ecb_infractions_by_tract = scale_and_group_tract(ecb_tract,'TRACT','INFRACTION_CODE1',\n",
    "                                       multiple=True,header_prefix='ECB_infraction_')\n",
    "\n",
    "# groupby tract: # permit type in tract / total permits in tract\n",
    "permits_by_tract = scale_and_group_tract(permits_tract,'TRACT','Permit Type',\n",
    "                                       multiple=True,header_prefix='DOB_permit_')\n",
    "\n",
    "# groupby tract: # oil or gas permits in tract / total permits in tract\n",
    "oil_gas_permits_by_tract = scale_and_group_tract(permits_tract,'TRACT','Oil Gas',\n",
    "                                       multiple=True,header_prefix='DOB_permit_',og=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process census data and aggregate by census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge race and income\n",
    "census = race.merge(income,how='outer',on='GEOID')\n",
    "\n",
    "census['TRACT'] = census.GEOID.apply(lambda x: str(x))\n",
    "\n",
    "# demo data by tract: race population / total population of tract\n",
    "for col in census.columns[3:9]:\n",
    "    census[col+'_ratio'] = census[col]*1.0/census['TOTAL_POPULATION']\n",
    "    del census[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge PLUTO datasets\n",
    "\n",
    "# age and building class\n",
    "merged_pluto = avg_bldg_age_by_tract.merge(bldgclass_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# merge land use\n",
    "merged_pluto = merged_pluto.merge(landuse_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# merge remaing PLUTO attributes\n",
    "merged_pluto = merged_pluto.merge(pluto_attrib_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# merge with FDNY\n",
    "pluto_fdny = merged_pluto.merge(fdny,how='left',on='TRACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge DOB / ECB data\n",
    "\n",
    "# dob complaints\n",
    "pluto_fdny_dob = pluto_fdny.merge(complaints_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# dob dispositions\n",
    "pluto_fdny_dob = pluto_fdny_dob.merge(disposition_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# dob violations\n",
    "pluto_fdny_dob = pluto_fdny_dob.merge(violations_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# ecb violations\n",
    "pluto_fdny_dob = pluto_fdny_dob.merge(ecb_violations_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# ecb infractions\n",
    "pluto_fdny_dob = pluto_fdny_dob.merge(ecb_infractions_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# dob permit type\n",
    "pluto_fdny_dob = pluto_fdny_dob.merge(permits_by_tract,how='left',on='TRACT')\n",
    "\n",
    "# dob oil or gas permit\n",
    "pluto_fdny_dob = pluto_fdny_dob.merge(oil_gas_permits_by_tract,how='left',on='TRACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge census data\n",
    "pluto_fdny_dob_census = pluto_fdny_dob.merge(census,how='left',on='TRACT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge spatial data\n",
    "nyc_spatial = zip_tracts_nyc[['NTACode','NTAName','geometry','TRACT','ZipCode']]\n",
    "\n",
    "master_merged = pluto_fdny_dob_census.merge(nyc_spatial,how='left',on='TRACT')\n",
    "\n",
    "# move target (i.e. dependent) variable to last column and rename \"total_gas_incidents\"\n",
    "master_merged['total_gas_incidents'] = master_merged['total_gas_incidents_yr'].astype(float)\n",
    " \n",
    "del master_merged['total_gas_incidents_yr']\n",
    "\n",
    "# add column gas incidents per building unit\n",
    "master_merged['gas_incidents_per_bldg_unit'] = master_merged[\n",
    "    'total_gas_incidents']*1.0/master_merged['total_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_merged.to_csv('processed_data/pluto_fdny_dob_census_to_tract_2014.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT: final merged \n",
    "\n",
    "#### PLUTO features\n",
    "- Avg building age per census tract\n",
    "- Ratio of each building class per census tract\n",
    "- Ratio of each land use per census tract\n",
    "- Building use ratio (commercial, residential, office, retail) per census tract\n",
    "- Residential unit density per census tract\n",
    "- Ave Unit area per census tract\n",
    "- Value per ft per census tract\n",
    "- Total units per census tract\n",
    "\n",
    "#### DOB/ECB features\n",
    "- Ratio of each DOB complaint type per census tract\n",
    "- Ratio of each DOB complaint disposition per census tract\n",
    "- Ratio of each DOB violation type per census tract\n",
    "- Ratio of each ECB violation type per census tract\n",
    "- Ratio of each DOB work permit type per census tract\n",
    "- Ratio of oil or gas permits out of all permits per census tract\n",
    "\n",
    "#### Census data features\n",
    "- Total population per census tract\n",
    "- Total households per census tract\n",
    "- Mean income census tract\n",
    "- Ratio of various racial groups out of total population per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
